---
#title: "generalizability-writeup"
format: 
  pdf:
    fontsize: 12pt
    mainfont: "Crimson"        # Primary text font
    CJKmainfont: "Noto Serif KR"  # Font for Korean text
    geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
    

header-includes:
- \usepackage{amsmath}
- \usepackage{fontspec}    % Ensure font support
- \usepackage{placeins}
- \usepackage{float}
- \usepackage{setspace}
- \usepackage{indentfirst}


echo: false
warning: false
message: false

editor: visual
bibliography: references.bib
csl: apa.csl
---

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(knitr)
library(kableExtra)
library(brms)
library(sjPlot)

fixed_effects_df_all = read_csv('../Data/fixed_effects_df_all.csv')
subject_effects = read_csv('../Data/subject_effects.csv')

fixed_effects_df_all$Task = factor(fixed_effects_df_all$Task, levels = c('prod', '2afc', 'comprehension'))

subject_effects$Task = factor(subject_effects$Task, levels = c('prod', '2afc', 'comprehension'))

condition_labels = c("cond1" = "Type Frequency", 
                      "cond2" = "Token Frequency", 
                      "cond3" = "Type-Token Frequency")
task_labels = c("prod" = "Production",
                "2afc" = "Form Choice",
                "comprehension" = "Comprehension")

fixed_effects_df_all$Meaning = factor(fixed_effects_df_all$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

subject_effects$Meaning = factor(subject_effects$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

fixed_effects_df_all$Frequency = factor(fixed_effects_df_all$Frequency, 
                                  levels = c("freq", "infreq"), 
                                  labels = c("Frequent", "Infrequent"))

subject_effects$Frequency = factor(subject_effects$Frequency, 
                                  levels = c("freq", "infreq"), 
                                  labels = c("Frequent", "Infrequent"))


# model_prod_original = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_original,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_original')
# 
# model_prod_novel = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_novel,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_novel')



model_prod_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both')



percent_greater_zero_both = data.frame(fixef(model_prod_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_prod_both = as.data.frame(fixef(model_prod_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_prod_both_stems = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both_stem')



percent_greater_zero_both_stems = data.frame(fixef(model_prod_both_stems, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both_stems = percent_greater_zero_both_stems %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_prod_both_stems = as.data.frame(fixef(model_prod_both_stems)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both_stems$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


rownames(fixefs_model_prod_both_stems) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem')


model_2afc_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_2afc_both')





percent_greater_zero_both = data.frame(fixef(model_2afc_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_2afc_both = as.data.frame(fixef(model_2afc_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_2afc_both) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel')





model_comprehension_numeric = brm(meaning ~ condition_numeric + (1 | participant),
                                       data = data_analysis_comprehension,
                                       family = bernoulli(link = 'logit'),
                                       iter = 10000, 
                                       warmup = 5000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       #control = list(adapt_delta = 0.99),
                                       file = '../Data/model_comprehension_numeric')

percent_greater_zero_comprehension_numeric = data.frame(fixef(model_comprehension_numeric, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


fixefs_model_comprehension_numeric = as.data.frame(fixef(model_comprehension_numeric)) %>%
    mutate(percent_greater_zero = percent_greater_zero_comprehension_numeric$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)

rownames(fixefs_model_comprehension_numeric) = c('Intercept', '12-12', '3-3')

```

# The effects of type and token frequency on semantic extension

\doublespacing

\setlength{\parindent}{4em}

## Methods

Following @harmonPuttingOldTools2017, two artificial languages were used: Dan and Nem [See @fig-fig1]. In each language, the same four suffixes were used: -*sil~PL~*, *-dan~PL~*, *-nem~DIM~*, and -*shoon~DIM~*. Notably, in our language *-dan* and *-sil* overlap in meaning (they both occur in plural contexts), and -*nem* and *-shoon* also overlap in meaning (they both occur in diminutive contexts). Since all four suffixes are possible candidates for the diminutive plural meaning, we will examine how properties of the language affect which suffixes are extended to express the diminutive plural meaning.

![A description of the suffixes in our artificial languages. The thicker lines denote the more frequent form in each language: the plural -*dan~PL~* in the Dan language and the diminutive -*nem~DIM~* in the Nem language.](languages.pdf){#fig-fig1}

During the exposure phase, each suffix was was paired with an image. The suffixes -*sil~PL~* and *-dan~PL~* were always paired with a picture of multiple large pictures. On the other hand, the suffixes *-nem~DIM~* and -*shoon~DIM~* were always paired with a picture of a single small creature. The design of the stimuli results in participants being able to learn that -*sil* and *-dan* are either simply plural or simply non-diminutive. Similarly, *-nem* and *-shoon* can be learned as either simply singular or simply diminutive.

Our Experiment comprised of three different conditions (see @tbl-conditionslist), one in which the type frequency of the frequent language's suffix was manipulated (Type Frequency), one in which the token frequency was manipulated (Token Frequency), and one in which both were manipulated (Type-Token Frequency). In this context, a higher type-frequency corresponds to the suffix appearing with a larger number of different stems relative to the competing suffix, while a larger token-frequency corresponds to simply appearing a greater number of times relative to the competing suffx, regardless of the number of different stems it occurs with.

```{r, echo = F, message = F}
#| label: tbl-conditionslist
#| tbl-cap: 'Description of each of our conditions. Note that in Condition 1, there are an equal number of tokens between the frequent and infrequent items, however there are a greater number of types in the frequent items. In Condition 2, the opposite is true: the frequent items occur more, but in the same number of types as the infrequent items. Finally, in Condition 3, the frequent items occur both a greater number of times and in a greater number of different contexts.'

conditionslist = data.frame('Frequent Token' = c(12, 12, 12),
                            'Frequent Type' = c(12, 3, 12),
                            'Infrequent Token' = c(12, 3, 3),
                            'Infrequent Type' = c(3, 3, 3))

conditionslist = conditionslist %>%
  mutate(term = rep(c('Type Frequency', 'Token Frequency', 'Type-Token Frequency'), times = 1))

conditionslist %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

### Procedure

Each participant was randomly assigned one of the conditions. In each condition, participants were first presented with an exposure phase. After the exposure phase, participants were tested using a production task, a form choice task, and a comprehension task. We describe each of these below.[^1]

[^1]: Additionally, a demo of the experiment can be found at the following link: <https://run.pavlovia.org/znhoughton/generalizability_demo>.

#### Exposure Phase

Following @harmonPuttingOldTools2017, each exposure trial consisted of the presentation of a picture on the computer screen which was subsequently followed with a written label for the image as well as an audio presentation of that label. Specifically, the image first appeared on the screen and then 1.25 seconds later was followed by both the label of the creatures on the screen as well as the audio for that creature. Participants were instructed to type the name of the creature and press enter. Participants had 4 seconds to respond after the presentation of the name of the creature and were given feedback as to whether they were correct or not.

Participants saw each trial within the exposure phase 5 times, each in a randomized order.

#### Production Task

After the exposure phase, participants were presented with a production task. In this task, participants were presented with images and told to produce a label for the image. Specifically, initially the unafixxed form appeared on the screen along with the corresponding image. 2 seconds later, the four different possible images of that creature appeared (a singular big creature, multiple big creatures, a single small creature, and multiple small creatures). Three of these images disappeared after 1.25 seconds, leaving a single image for the participant to produce a label for. Participants had 10 seconds to respond. In the production task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

Participants saw each trial within the production task 4 times, each in a randomized order.

#### Form Choice Task

After the production task, participants were presented with a form choice task. In this task, participants were presented first with the base, unaffixed form and the corresponding image. 2 seconds later four images flashed on the screen, remained on the screen for 1.25 seconds, and disappeared leaving a single image. Along with the single image, participants were also presented with two possible labels for that image, one label on the bottom right of the screen and one label on the bottom left of the screen. Participants were given four seconds to press either the left arrow or the right arrow to choose the corresponding label. The labels were counterbalanced with respect to which side of the screen they appeared on. In the form choice task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training). The goal of this task was to assess whether type and/or token frequency influence the form choice when accessibility differences between frequent and rare forms have been attenuated.

Participants saw each trial within the form choice task 2 times, each in a randomized order.

#### Comprehension Task

Finally, participants were presented with a comprehension task. In this task, participants were first given the label and corresponding audio for a given creature. After 0.25 seconds, four images appeared on the screen and participants had 4 seconds to click one of the images on the screen that corresponded to the label. Similar to the before-mentioned tasks, in the comprehension task some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

Participants saw each trial within the comprehension task 2 times, each in a randomized order.

## Analyses and Results

A plot of our model estimates for each task is presented in @fig-fullresults. The points represent individual subject estimates, the bars and credible intervals correspond to estimates from a Bayesian linear mixed-effects regression model.[^2]

[^2]: All data and code for the analyses can be found here: <https://github.com/znhoughton/Generalizability-Type-Token>.

### Production Task

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullresults
#| fig-cap: "Plot of our results. Points indicate individual subject values."
#| fig-height: 6
#| fig-width: 7

ggplot(fixed_effects_df_all, aes(x = Meaning, y = Prob, fill = Frequency)) +
  geom_col(position = position_dodge(width = 0.9)) + 
  geom_errorbar(aes(ymax = Upper, ymin = Lower), width = 0.2, position = position_dodge(width = 0.9)) +
  geom_point(data = subject_effects, 
             aes(x = Meaning, y = subj_values, fill = Frequency, color = Frequency),  # Map fill and color
             shape = 21,   # Hollow circle with fill and outline
             position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.9), 
             size = 2, stroke = 0.7, alpha = 0.1,  # Adjust size, border thickness, and transparency
             show.legend = FALSE) +  # Hides points from legend
  scale_fill_manual(values = c("Frequent" = "#66c2a5", "Infrequent" = "#8da0cb")) +  # Bar & point fill colors
  scale_color_manual(values = c("Frequent" = "#1b7837", "Infrequent" = "#2b8cbe")) +  # Darker border colors for points
  facet_grid(Task ~ Condition, labeller = labeller(Condition = condition_labels, Task = task_labels)) + 
  theme_bw() +
  ggtitle('Results by condition for each task') +
  xlab('Condition') +
  ylab('Response Probability') +
  ylim(0, 1)
```

<!--# Freq bar in original is choosing dan for big_pl or nem for dim_sg in the dan language and infreq bar is choosing dan for big_pl and nem for dim_sg in the nem language -->

In order to examine the effects of type and token-frequency on participants choice of suffix, we examined the effects of type and token-frequency on the original meaning as well as the novel meaning (diminutive plural). Specifically, we ran a Bayesian linear mixed-effects model on the production data. The dependent variable was whether the participant produced the frequent suffix (*dan* in the *dan* language, *nem* in the *nem* language). A larger estimate indicates that the frequent suffix was more likely to be produced than the infrequent suffix. The independent variables were condition, meaning, and stem (either familiar stem or novel stem). Condition was treatment coded such that the intercept was the high type and high token frequency condition. Meaning was sum coded. We also included a random intercept for participant and a random slope for meaning by participant. The equation for our model is included below in @eq-prodmodels.

$$
\text{frequent\_suffix} \sim \text{condition}*\text{meaning} + (1 + \text{meaning} | \text{participant})
$$ {#eq-prodmodels}

The results are below in @tbl-prodresults. The results suggest that in general the frequent suffix is used more than the infrequent suffix for all conditions. Notably however, we find no meaningful interaction effect for type frequency and novel meaning, suggesting that the effect of meaning is similar for both conditions. We do, however, find an interaction effect between token frequency and novel meaning, suggesting that the frequent suffix is used less for the novel meaning in the token frequency condition. This suggests that there is semantic extension when there is high type and high token frequency as well as the case where there is high type-frequency and low token-frequency, but in the case where there is high token-frequency with a low type-frequency, we see entrenchment in production.

```{r, echo = F, message = F}
#| label: tbl-prodresults
#| tbl-cap: 'Results of the statistical models for the production task.'

prod_both_meanings = bind_rows(fixefs_model_prod_both, .id = "ID") %>%
  mutate(term = rep(c('Intercept (Type-Token Frequency)', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel'), times = 1))


prod_both_meanings = prod_both_meanings %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

prod_both_meanings %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')
```

In order to follow up on this, we ran an additional model that included whether the stem was familiar (occurred in training) or novel (did not occur in training). The results are reported in @tbl-prodresultsstem and visualized in @fig-prodresultsstem.

Our follow-up analysis suggests that type-frequency leads to entrenchment with stems that learners have encountered before. However, for novel stems, participants are more likely to use the frequent suffix to convey a novel meaning.

```{r, echo = F, message = F}
#| label: tbl-prodresultsstem
#| tbl-cap: 'Results of the statistical models for the production task.'

prod_both_meanings_stems = bind_rows(fixefs_model_prod_both_stems, .id = "ID") %>%
  mutate(term = rep(c('Intercept (Type-Token Frequency)', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1))


prod_both_meanings_stems = prod_both_meanings_stems %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

prod_both_meanings_stems %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '20em')
```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-prodresultsstem
#| fig-cap: "Plot of the statistical model estimates for the production task."
#| fig-height: 5
#| fig-width: 7


plot_prod_stem = plot_model(model_prod_both_stems, type = 'int',
                            axis.title = c('Condition', 'Frequency'),
                            axis.labels = "condition",
                            legend.title = 'Meaning',
                            title = "",
                            dot.size = 2,
                            line.size = 1,
                            colors = c('#298c8c', '#800074'))[[4]] 
plot_prod_stem[[11]]$linetype = 'Meaning'
plot_prod_stem[[11]]$shape = 'Meaning'

plot_prod_stem[[1]]$group_col = factor(plot_prod_stem[[1]]$group_col, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$group = factor(plot_prod_stem[[1]]$group, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$facet = factor(plot_prod_stem[[1]]$facet, levels = c('novel', 'familiar'), labels = c('Novel Stem', 'Familiar Stem'))

#plot_prod_stem[[1]]$x

plot_prod_stem +
  scale_x_continuous(breaks = c(1, 2, 3), 
                     labels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency')) +
  ylab('Proportion of Responses with Frequent Suffix') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black', size = 10),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        axis.text.y = element_text(color = 'black', size = 10)) 

```

### Form Choice Task

In order to examine the effects of form-choice, we similarly ran a model analogous to the first model we ran for the production task (@eq-prodmodels). In the context of the form-choice task, the dependent variable reflects whether participants chose the option with the frequent suffix or the one with the infrequent suffix.

The results of the form-choice task suggest that when participants are presented with both possible options, for both token-frequency and type-token frequency, participants are equally likely to choose the frequent suffix as the infrequent suffix. Interestingly, for the type-frequency condition, participants were less likely to choose the novel meaning than the original meaning, though the effect size is quite small and their selections were still close to chance.[^3]

[^3]: We ran an additional model with stem condition as a fixed-effect (along with its interaction with condition and meaning) and found no effect of stem condition.

```{r, echo = F, message = F}
#| label: tbl-2afcresults
#| tbl-cap: 'Results of the statistical models for the 2afc task.'
#| 
tafc_both_meanings = bind_rows(fixefs_model_2afc_both, .id = "ID") %>%
  mutate(term = rep(c('Intercept (Type-Token Frequency)', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel'), times = 1))


tafc_both_meanings = tafc_both_meanings %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(tafc_both_meanings$meaning)

tafc_both_meanings %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')

```

### Comprehension Task

Our results for the comprehension task are included below in @tbl-comprehensionresults. For the comprehension task, the dependent variable for our statistical analysis was whether the meaning that participants chose was novel or original. A positive estimate suggests that participants were more likely to choose the novel meaning, a negative estimate suggests that participants were more likely to choose the original meaning.

For comprehension, our independent variables were different from the other tasks. Specifically, to maximize our power, we divided the data up based on whether the token-to-type ratio was 12-12, 12-3, or 3-3. For example, in the Type Frequency condition, while the frequent suffix occurs 12 times with 12 different types, the infrequent occurs 12 times in only 3 different types. By comparing the 12-3 condition to the 12-12 and 3-3 conditions, we can see how an increase in type-frequency and a decrease in token-frequency affect participants' choice of the novel or original meaning.

We treatment coded condition such that the intercept was the 12-3 condition. As such, for each condition, a positive coefficient indicates that participants were more likely to choose a novel meaning (relative to the 12-3 condition/intercept). The model equation is included below in @eq-comprehensionmodel.[^4]

[^4]: Similar to form choice, we also ran a model with stem condition as a fixed-effect and found no effect.

$$
\text{meaning} \sim \text{condition\_numeric} + (1 | \text{participant})
$$ {#eq-comprehensionmodel}

First, we find a meaningful effect for the intercept (12-3 condition), suggesting that participants are more likely to select the original meaning when there is a high token-to-type ratio. Additionally, we find positive coefficient estimates for both 12-12 and 3-3 suggesting that when there is an equal number of tokens as types, participants are more likely to select the novel meaning. In other words, entrenchment seems to be driven not simply by the raw value of token or type frequency, but rather the proportion of tokens to types. That is, it is the relationship between type-frequency and token-frequency that is relevant for semantic entrenchment in comprehension. This mirrors the results for our production task where a high type and high token frequency leads to semantic extension, but having a high token frequency with low type frequency leads to entrenchment.

```{r, echo = F, message = F}
#| label: tbl-comprehensionresults
#| tbl-cap: 'Results of the statistical models for the comprehension task.'
#| 

fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(term = rep(c('Intercept (12-3)', '12-12', '3-3'), times = 1))


fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_model_comprehension_numeric %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

<!--# there's probably a note about storage here -->

## Discussion

Our results suggest that in production, having a high type and high token frequency (relative to the competitor), along with simply having a high type but equal token frequency (relative to the competitor), leads to semantic extension, while having a high type frequency but low token frequency leads to entrenchment (i.e., using the suffix more with the original meaning than novel meanings). Further, this mirrors what we see in comprehension where participants are more likely to choose the novel meaning when the type and token frequencies are matched, but less likely when the suffix has high token frequency but low type frequency. Further, when both forms are made accessible as in the form choice task, this effect is reduced for all conditions, though less-so for the type-frequency condition.

\newpage
