---
#title: "generalizability-writeup"
format: 
  pdf:
    fontsize: 12pt
    mainfont: "Crimson"        # Primary text font
    CJKmainfont: "Noto Serif KR"  # Font for Korean text
    geometry:
      - left=1in
      - right=1in
      - top=1in
      - bottom=1in
    

header-includes:
- \usepackage{amsmath}
- \usepackage{fontspec}    % Ensure font support
- \usepackage{placeins}
- \usepackage{float}
- \usepackage{setspace}
- \usepackage{indentfirst}


echo: false
warning: false
message: false

editor: visual
bibliography: references.bib
csl: apa.csl
---

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(knitr)
library(kableExtra)
library(brms)
library(sjPlot)


fixed_effects_df_all = read_csv('../Data/fixed_effects_df_all.csv')
subject_effects = read_csv('../Data/subject_effects.csv')

fixed_effects_df_all$Task = factor(fixed_effects_df_all$Task, levels = c('prod', '2afc', 'comprehension'))

subject_effects$Task = factor(subject_effects$Task, levels = c('prod', '2afc', 'comprehension'))



condition_labels = c("cond1" = "Type Frequency", 
                      "cond2" = "Token Frequency", 
                      "cond3" = "Type-Token Frequency")
task_labels = c("prod" = "Production",
                "2afc" = "Form-Choice",
                "comprehension" = "Comprehension")

fixed_effects_df_all$Meaning = factor(fixed_effects_df_all$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

subject_effects$Meaning = factor(subject_effects$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

fixed_effects_df_all$Stem = factor(fixed_effects_df_all$Stem, 
                                  levels = c("familiar", "novel"), 
                                  labels = c("Familiar", "Novel"))

subject_effects$Stem = factor(subject_effects$Stem, 
                                  levels = c("familiar", "novel"), 
                                  labels = c("Familiar", "Novel"))


# model_prod_original = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_original,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_original')
# 
# model_prod_novel = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_novel,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_novel')



model_prod_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both')



percent_greater_zero_both = data.frame(fixef(model_prod_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_prod_both = as.data.frame(fixef(model_prod_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_prod_both_stems = brm(frequency ~ condition*meaning*stem_condition + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both_stem')



percent_greater_zero_both_stems = data.frame(fixef(model_prod_both_stems, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both_stems = percent_greater_zero_both_stems %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1')))


fixefs_model_prod_both_stems = as.data.frame(fixef(model_prod_both_stems)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both_stems$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


rownames(fixefs_model_prod_both_stems) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem')


model_2afc_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_2afc_both')


percent_greater_zero_both = data.frame(fixef(model_2afc_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_2afc_both = as.data.frame(fixef(model_2afc_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_2afc_both) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel')



model_2afc_both_stem = brm(frequency ~ condition*meaning*stem_condition + (1 + meaning | participant),
                                       data = data_2afc_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_2afc_both_stem')




percent_greater_zero_both = data.frame(fixef(model_2afc_both_stem, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1')))


fixefs_model_2afc_both_stem = as.data.frame(fixef(model_2afc_both_stem)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_2afc_both_stem) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem')





model_comprehension_numeric = brm(meaning ~ condition_numeric + (1 | participant),
                                       data = data_analysis_comprehension,
                                       family = bernoulli(link = 'logit'),
                                       iter = 10000, 
                                       warmup = 5000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       #control = list(adapt_delta = 0.99),
                                       file = '../Data/model_comprehension_numeric')

percent_greater_zero_comprehension_numeric = data.frame(fixef(model_comprehension_numeric, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_comprehension_numeric = percent_greater_zero_comprehension_numeric %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition_numeric12M12', 'condition_numeric3M3')))


fixefs_model_comprehension_numeric = as.data.frame(fixef(model_comprehension_numeric)) %>%
    mutate(percent_greater_zero = percent_greater_zero_comprehension_numeric$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_comprehension_numeric) = c('Intercept', '12-12', '3-3')


model_comprehension_numeric_stem = brm(meaning ~ condition_numeric*stem_condition + (1 | participant),
                                       data = data_analysis_comprehension,
                                       family = bernoulli(link = 'logit'),
                                       iter = 10000, 
                                       warmup = 5000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       #control = list(adapt_delta = 0.99),
                                       file = '../Data/model_comprehension_numeric_stem_test')

percent_greater_zero_comprehension_numeric_stem = data.frame(fixef(model_comprehension_numeric_stem, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_comprehension_numeric_stem = percent_greater_zero_comprehension_numeric_stem %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition_numeric12M12', 'condition_numeric3M3', 'stem_condition1', 'condition_numeric12M12.stem_condition1', 'condition_numeric3M3.stem_condition1')))



fixefs_model_comprehension_numeric_stem = as.data.frame(fixef(model_comprehension_numeric_stem)) %>%
    mutate(percent_greater_zero = percent_greater_zero_comprehension_numeric_stem$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_comprehension_numeric_stem) = c('Intercept', '12-12', '3-3', 'Novel Stem', '12-12:Novel Stem', '3-3:Novel Stem')


plot_df_stems_all_cond1 = read_csv('../Data/plot_df_stems_all_cond1_linear.csv')
plot_df_stems_all_cond2 = read_csv('../Data/plot_df_stems_all_cond2_linear.csv')
plot_df_stems_all_cond3 = read_csv('../Data/plot_df_stems_all_cond3_linear.csv')

plot_df_stems_cond1 = ggplot(plot_df_stems_all_cond1, aes(x = row_number, y = weights, color = label)) +
  geom_line() +
  labs(
    x = "Index",
    y = "Activation Weight",
    color = "Meaning and Frequency",
    title = "Type Frequency Condition"
  ) +
  ylim(-1.5,2) +
  facet_wrap(~Stem) +
  theme_minimal()

plot_df_stems_cond2 = ggplot(plot_df_stems_all_cond2, aes(x = row_number, y = weights, color = label)) +
  geom_line() +
  labs(
    x = "Index",
    y = "Activation Weight",
    color = "Meaning and Frequency",
    title = "Token Frequency Condition"
  ) +
  ylim(-1.5,2) +
  facet_wrap(~Stem) +
  theme_minimal()

plot_df_stems_cond3 = ggplot(plot_df_stems_all_cond3, aes(x = row_number, y = weights, color = label)) +
  geom_line() +
  labs(
    x = "Index",
    y = "Activation Weight",
    color = "Meaning and Frequency",
    title = "Type-Token Frequency Condition"
  ) +
  ylim(-1.5,2) +
  facet_wrap(~Stem) +
  theme_minimal()

all_cond_estimates_paper = read_csv('../Data/all_cond_estimates_linear.csv')

all_cond_estimates_paper$Condition = factor(all_cond_estimates_paper$Condition, levels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency'))

all_cond_estimates_paper2 = read_csv('../Data/all_cond_estimates_for_paper2.csv')

all_cond_estimates_paper2$Condition = factor(all_cond_estimates_paper2$Condition, levels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency'))




model_prod_both_stem_rw = brm(frequency ~ condition*meaning*stem_condition + freq_minus_infreq + (1 + stem_condition*meaning | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_both_stem_rw_preds_final')


percent_greater_zero_rw_preds = data.frame(fixef(model_prod_both_stem_rw, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_rw_preds = percent_greater_zero_rw_preds %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'freq_minus_infreq', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1')))


fixefs_model_rw_preds = as.data.frame(fixef(model_prod_both_stem_rw)) %>%
    mutate(percent_greater_zero = percent_greater_zero_rw_preds$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)

#fixefs_model_rw_preds

rownames(fixefs_model_rw_preds) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Frequent - Infrequent (RW)', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem')


model_human_rw_preds = brm(frequency ~ freq_minus_infreq + (1 | participant) + (1|resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds,
                                       family = bernoulli(link = 'logit'),
                                       iter = 15000, 
                                       warmup = 7500,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       #control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_rw_preds')

model_prod_both_stem_rw_simple = brm(frequency ~ condition*meaning*stem_condition + freq_minus_infreq + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_both_stem_rw_simple')


model_prod_both_stem_simple_humans = brm(frequency ~ condition*meaning*stem_condition + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_both_stem_simple_humans')

model_prod_rw_stem_suffix_vary = brm(frequency ~ freq_minus_infreq_stem + freq_minus_infreq_suffix + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_stem_suffix_vary,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_rw_stem_suffix_vary')

percent_greater_zero_rw_preds_stem_suffix_vary = data.frame(fixef(model_prod_rw_stem_suffix_vary, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_rw_preds_stem_suffix_vary = percent_greater_zero_rw_preds_stem_suffix_vary %>%
  arrange(match(beta_coefficient, c('Intercept', 'freq_minus_infreq_stem', 'freq_minus_infreq_suffix')))


fixefs_model_rw_preds_stem_suffix_vary = as.data.frame(fixef(model_prod_rw_stem_suffix_vary)) %>%
    mutate(percent_greater_zero = percent_greater_zero_rw_preds_stem_suffix_vary$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_prod_rw_stem_suffix_vary_by_condition = brm(frequency ~ freq_minus_infreq_stem + freq_minus_infreq_suffix + condition + freq_minus_infreq_stem:condition + freq_minus_infreq_suffix:condition + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_stem_suffix_vary,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_rw_stem_suffix_vary_by_condition')

percent_greater_zero_rw_preds_stem_suffix_vary_by_condition = data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_rw_preds_stem_suffix_vary_by_condition = percent_greater_zero_rw_preds_stem_suffix_vary_by_condition %>%
  arrange(match(beta_coefficient, c('Intercept', 'freq_minus_infreq_stem', 'freq_minus_infreq_suffix', 'condition1', 'condition2', 'freq_minus_infreq_stem.condition1', 'freq_minus_infreq_stem.condition2', 'freq_minus_infreq_suffix.condition1', 'freq_minus_infreq_suffix.condition2')))


fixefs_model_rw_preds_stem_suffix_vary_by_condition = as.data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition)) %>%
    mutate(percent_greater_zero = percent_greater_zero_rw_preds_stem_suffix_vary_by_condition$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_prod_rw_stem_suffix_vary_by_condition_original_preds = brm(frequency ~ condition*meaning*stem_condition + freq_minus_infreq_stem + freq_minus_infreq_suffix + freq_minus_infreq_stem:condition + freq_minus_infreq_suffix:condition + (1 | participant) + (1 | resp_stem), 
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_stem_suffix_vary,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_prod_rw_stem_suffix_vary_by_condition_original_preds')

percent_greater_zero_rw_preds_stem_suffix_vary_by_condition_original_preds = data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition_original_preds, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_rw_preds_stem_suffix_vary_by_condition_original_preds = percent_greater_zero_rw_preds_stem_suffix_vary_by_condition_original_preds %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'freq_minus_infreq_stem', 'freq_minus_infreq_suffix', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.freq_minus_infreq_stem', 'condition2.freq_minus_infreq_stem', 'condition1.freq_minus_infreq_suffix', 'condition2.freq_minus_infreq_suffix', 'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1'))) 


fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds = as.data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition_original_preds)) %>%
    mutate(percent_greater_zero = percent_greater_zero_rw_preds_stem_suffix_vary_by_condition_original_preds$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_human_conf_sem_preds = brm(frequency ~ freq_minus_infreq + (1 | participant) + (1|resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_sem_alpha,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_conf_sem_preds')


percent_greater_zero_model_human_conf_sem_preds = data.frame(fixef(model_human_conf_sem_preds, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_model_human_conf_sem_preds = percent_greater_zero_model_human_conf_sem_preds %>%
  arrange(match(beta_coefficient, c('Intercept', 'freq_minus_infreq_stem', 'freq_minus_infreq_suffix', 'freq_minus_infreq_stem.freq_minus_infreq_suffix')))


fixefs_model_model_human_conf_sem_preds = as.data.frame(fixef(model_human_conf_sem_preds)) %>%
    mutate(percent_greater_zero = percent_greater_zero_model_human_conf_sem_preds$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_human_conf_sem_preds_original_preds = brm(frequency ~ freq_minus_infreq + condition*meaning*stem_condition + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_sem_alpha,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_conf_sem_preds_original_preds')


loo_co_results = data.frame(loo_compare(model_human_rw_preds, model_prod_rw_stem_suffix_vary, model_prod_both_stem_rw_simple, model_prod_both_stem_simple_humans, model_prod_rw_stem_suffix_vary_by_condition, model_prod_rw_stem_suffix_vary_by_condition_original_preds)) %>%
  mutate('term' = c('Original Predictors Plus RW Activations', 'Original Statistical Predictors', 'Stem and Meaning Activations by Condition Plus Original Predictors', 'Stem and Meaning Activations by Condition', 'Stem and Meaning Activations as Separate Predictors', 'Only RW Activation')) 


  




fixed_effects_df = read_csv('../Data/fixed_effects_df_all.csv') %>%
  filter(Task == 'prod')

observed_estimates = fixed_effects_df 


observed_estimates_plot = ggplot(data = observed_estimates, aes(x = Meaning, y = Prob, fill = Stem)) +
  geom_col(position = "dodge") +
  ylim(c(0,1)) +
  facet_wrap(~Condition)




model_human_conf = brm(frequency ~ freq_minus_infreq + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_conf,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_conf')


model_human_conf_original_preds = brm(frequency ~ freq_minus_infreq + condition*meaning*stem_condition + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_conf,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_conf_original_preds')


model_human_mod_sem_alpha = brm(frequency ~ freq_minus_infreq + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_nonconf_sem_alpha,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_mod_sem_alpha')

model_human_mod_sem_alpha_original_preds = brm(frequency ~ freq_minus_infreq + condition*meaning*stem_condition + (1 | participant) + (1 | resp_stem),
                                       data = human_data_no_sil_no_shoon_plus_rw_preds_nonconf_sem_alpha,
                                       family = bernoulli(link = 'logit'),
                                       iter = 22000, 
                                       warmup = 1100,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       save_pars = save_pars(all = TRUE),
                                       file = '../Data/model_human_mod_sem_alpha_original_preds')



#loo_co_results_add = data.frame(loo_compare(model_human_rw_preds, model_prod_rw_stem_suffix_vary, model_prod_both_stem_rw_simple, model_prod_both_stem_simple_humans, model_prod_rw_stem_suffix_vary_by_condition, model_prod_rw_stem_suffix_vary_by_condition_original_preds, model_human_conf_sem_preds)) %>%
  #mutate('term' = c('Configural Cues and Increased Saliency of Semantic Cues', 'Original Predictors Plus RW Activations', 'Original Statistical Predictors', 'Stem and Meaning Activations by Condition Plus Original Predictors', 'Stem and Meaning Activations by Condition', 'Stem and Meaning Activations as Separate Predictors', 'Only RW Activation')) 

loo_co_results_add = data.frame(loo_compare(model_human_rw_preds, model_prod_both_stem_simple_humans, model_prod_rw_stem_suffix_vary_by_condition, model_human_conf_sem_preds, model_human_conf, model_human_conf_sem_preds_original_preds, model_human_mod_sem_alpha)) %>%
  mutate('term' = c('Configural Cues and Increased Saliency of Semantic Cues',
                    'Configural Cues, Increased Saliency of Semantic Cues, and Original Statistical Predictors',
                    'Only Increased Saliency of Semantic Cues',
                    'Only Configural Cues',
                    'Original Statistical Predictors', 
                    'Stem and Meaning Activations by Condition',
                    'Only RW Activation')) 


```

# The effects of type and token frequency on semantic extension

\doublespacing

\setlength{\parindent}{4em}

## Introduction

## Methods

Following @harmonPuttingOldTools2017, two artificial languages were used: called Dan and Nem (See @fig-fig1). In each language, the same four suffixes were used: -*sil~PL~*, *-dan~PL~*, *-nem~DIM~*, and -*shoon~DIM~*. Notably, in our language *-dan* and *-sil* overlap in meaning (they both occur in plural contexts), and -*nem* and *-shoon* also overlap in meaning (they both occur in diminutive contexts). Since all four suffixes are possible candidates for the diminutive plural meaning, we can examine how properties of the language (mainly type frequency and token frequency) affect which suffixes are extended to express the diminutive plural meaning.

![A description of the suffixes in our artificial languages. The thicker lines denote the more frequent form in each language: the plural -*dan~PL~* in the Dan language and the diminutive -*nem~DIM~* in the Nem language.](languages.pdf){#fig-fig1}

During the exposure phase, each suffix was paired with an image. The suffixes -*sil~PL~* and *-dan~PL~* were always paired with a picture of multiple large pictures. On the other hand, the suffixes *-nem~DIM~* and -*shoon~DIM~* were always paired with a picture of a single small creature. The design of the stimuli results in participants being able to learn that -*sil* and *-dan* are either simply plural or simply non-diminutive. Similarly, *-nem* and *-shoon* can be learned as either simply singular or simply diminutive.

Our Experiment comprised of three different conditions (see @tbl-conditionslist), one in which the type frequency of the frequent suffix (i.e., *dan* in Dan and *nem* in Nem) was higher than the type frequency of the infrequent suffix (by a factor of 4 or 12 vs. 3) while the token frequency of the two suffixes was matched and set equal to the type frequency of the frequent suffix (both at 12). We refer to this condition as Type. In the second condition, the token frequency of the frequent suffix was higher than the token frequency of the infrequent suffix (by a factor of 4 or 12 vs. 3) while holding the type frequency of the two suffixes equal to the type frequency of the infrequent suffix (both at 3). We refer to this condition as Token. Finally, in the third condition, we manipulated both the type and token frequency of the frequent suffix such that the token and type frequency of the frequent suffix were both greater than the token and type frequency of the infrequent suffix (by a factor of 4 or 12 vs. 3). We refer to this condition as Type-Token. In this context, a higher type frequency corresponds to the suffix appearing with a larger number of distinct stems relative to the competing suffix, while a larger token frequency corresponds to simply appearing a greater number of times relative to the competing suffix, regardless of the number of different stems it occurs with.

```{r, echo = F, message = F}
#| label: tbl-conditionslist
#| tbl-cap: 'Description of each of our conditions. Note that in Condition 1, there are an equal number of tokens between the frequent and infrequent items, however there are a greater number of types in the frequent items. In Condition 2, the opposite is true: the frequent items occur more, but in the same number of types as the infrequent items. Finally, in Condition 3, the frequent items occur both a greater number of times and in a greater number of different contexts.'

# conditionslist = data.frame('Frequent Token' = c(12, 12, 12),
#                             'Frequent Type' = c(12, 3, 12),
#                             'Infrequent Token' = c(12, 3, 3),
#                             'Infrequent Type' = c(3, 3, 3))
# 
# conditionslist = conditionslist %>%
#   mutate(term = rep(c('Type', 'Token', 'Type-Token'), times = 1))
# 
# conditionslist %>%
#   dplyr::select(term, everything()) %>%
#   rename(` ` = term) %>%
#   kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
#   kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
#   #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
#   row_spec(0, bold = TRUE) %>%
#   column_spec(1, bold = T, width = '12em')
# 
# 

# your data
conditionslist = data.frame(
  'Frequent Token'   = c(12, 12, 12),
  'Frequent Type'    = c(12,  3, 12),
  'Infrequent Token' = c(12,  3,  3),
  'Infrequent Type'  = c( 3,  3,  3)
)

# rename/reshape for clarity:
# - make a "Condition" column with values Type / Token / Type-Token
# - order columns so headers can group as: Frequent(Type, Token), Infrequent(Type, Token)
tbl = conditionslist %>%
  mutate(Condition = c("Type", "Token", "Type-Token")) %>%
  select(
    Condition,
    `Frequent.Type`, `Frequent.Token`,
    `Infrequent.Type`, `Infrequent.Token`
  )

# build the table with two header rows
tbl %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    digits = 2,
    row.names = FALSE,
    escape = FALSE,        # allow LaTeX header spanning
    col.names = c("Condition", "Type", "Token", "Type", "Token")
  ) %>%
  add_header_above(c(" " = 1, "Frequent" = 2, "Infrequent" = 2)) %>%
  kable_styling(
    latex_options = c("HOLD_position", "striped"),
    font_size = 11,
    full_width = FALSE
  ) %>%
  row_spec(0, bold = TRUE) %>%     # header row bold
  column_spec(1, bold = TRUE, width = "12em")

```

### Procedure

Each participant was randomly assigned one of the conditions. In each condition, participants were first presented with an exposure phase. After the exposure phase, participants were tested using a production task, a form choice task, and a comprehension task. We describe each of these below.[^1]

[^1]: Additionally, a demo of the experiment can be found at the following link: <https://run.pavlovia.org/znhoughton/generalizability_demo>.

#### Exposure Phase

Following @harmonPuttingOldTools2017, each exposure trial consisted of the presentation of a picture on the computer screen which was subsequently followed with a written label for the image as well as an audio presentation of that label. Specifically, the image first appeared on the screen and then 1.25 seconds later was followed by both the label of the creatures on the screen as well as the audio for that creature. Participants were instructed to type the name of the creature and press enter. Participants had 4 seconds to respond after the presentation of the name of the creature and were given feedback as to whether they were correct or not.

Participants saw each trial within the exposure phase 5 times, each in a randomized order.

#### Production Task

After the exposure phase, participants were presented with a production task. In this task, participants were presented with images and told to produce a label for the image. Specifically, initially the unaffixed form appeared on the screen along with the corresponding image. 2 seconds later, the four different possible images of that creature appeared (a singular big creature, multiple big creatures, a single small creature, and multiple small creatures). Three of these images disappeared after 1.25 seconds, leaving a single image for the participant to produce a label for. Participants had 10 seconds to respond. In the production task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

Participants saw each trial within the production task 4 times, each in a randomized order.

#### Form Choice Task

After the production task, participants were presented with a form choice task. In this task, participants were presented first with the base, unaffixed form and the corresponding image. 2 seconds later four images flashed on the screen, remained on the screen for 1.25 seconds, and disappeared leaving a single image. Along with the single image, participants were also presented with two possible labels for that image, one label on the bottom right of the screen and one label on the bottom left of the screen. Participants were given four seconds to press either the left arrow or the right arrow to choose the corresponding label. The labels were counterbalanced with respect to which side of the screen they appeared on. In the form choice task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training). The goal of this task was to assess whether type and/or token frequency influence the form choice when accessibility differences between frequent and rare forms have been attenuated.

Participants saw each trial within the form choice task 2 times, each in a randomized order.

#### Comprehension Task

Finally, participants were presented with a comprehension task. In this task, participants were first given the label and corresponding audio for a given creature. After 0.25 seconds, four images appeared on the screen and participants had 4 seconds to click one of the images on the screen that corresponded to the label. Similar to the before-mentioned tasks, in the comprehension task some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

Participants saw each trial within the comprehension task 2 times, each in a randomized order.

## Analyses and Results

```{r eval = F}
# Before going into detail about each task, we first present a plot of our overall results  (@fig-fullresults)[^2]. In order to visualize the results, we subsetted the data by condition (type frequency, token frequency, or type-token frequency), stem (familiar or novel), task (production, form-choice, or comprehension), and meaning (original or novel). We then ran a logistic regression model. For the form-choice and production tasks, the dependent variable was 1 if the participant chose the frequent suffix for that trial or 0 if they chose the infrequent suffix. The model included an intercept as well as random intercepts for participant and stem identity. The frequent suffix was operationalized differently depending on the meaning of the item. For novel meanings (diminutive plural), the frequent suffix was simply whether they chose 
# *dan* in the Dan language or *nem* in the Nem language. However, for original meanings (big plural and diminutive singular), the frequent suffix was 1 if they chose *dan* for big plural and 0 if they chose *nem* for diminutive singular in the Dan language, and vice versa in the nem language.
# 
# For the comprehension task, the dependent variable was 1 if they chose the original meaning for the original meaning portion and 1 if they chose the novel meaning in the novel meaning portion and the independent variable was the intercept with random intercepts for participant and stem identity. For example, the larger estimates for the original meaning in the type frequency condition indicate that participants preferred to select the original meaning for both stem types. As a result, these bars are lower for the novel meaning since the original and novel estimates for a given stem type sum to 1. While this visualization for the comprehension is admittedly a bit awkward, the advantage is it allows us to visualize it alongside the results for production and form-choice tasks.
```





```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F, eval = F}
#| label: fig-fullresults
#| fig-cap: "Plot of our results. Points indicate individual subject values. The x-axis indicates whether the meaning was original or novel. Green coloring corresponds to the frequent stem while blue corresponds to the infrequent stem. The y-axis indicates the response probability. For Production and Form-choice, it is the response probability of choosing the frequent form. Thus, a value closer to 1 indicates that participants chose the frequent form more than the infrequent form. For Comprehension, it is the response probability of choosing a given meaning. For example, a value closer to 1 for the original meaning with a novel stem indicates that when participants saw a novel stem with *dan*, they were more likely to select the big.pl meaning than the dim.pl meaning (as a result, for a given stem familiarity the original and novel bars sum to 1 in the comprehension condition). Facets indicate condition (type frequency, token frequency, or type-token frequency) and task (production, form-choice, comprehension). The results were obtained by running a separate regression model "
#| fig-height: 6
#| fig-width: 7

ggplot(fixed_effects_df_all, aes(x = Meaning, y = Prob, fill = Stem)) +
  geom_col(position = position_dodge(width = 0.9)) + 
  geom_point(data = subject_effects, 
             aes(x = Meaning, y = subj_values, fill = Stem, color = Stem),  # Map fill and color
             shape = 21,   # Hollow circle with fill and outline
             position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.9), 
             size = 2, stroke = 0.7, alpha = 0.1,  # Adjust size, border thickness, and transparency
             show.legend = FALSE) +  # Hides points from legend
  scale_fill_manual(values = c("Familiar" = "#66c2a5", "Novel" = "#8da0cb")) +  # Bar & point fill colors
  scale_color_manual(values = c("Familiar" = "#1b7837", "Novel" = "#2b8cbe")) +  # Darker border colors for points
  facet_grid(Task ~ Condition, labeller = labeller(Condition = condition_labels, Task = task_labels)) + 
  geom_errorbar(aes(ymax = Upper, ymin = Lower), width = 0.2, position = position_dodge(width = 0.9)) +
  theme_bw() +
  ggtitle('Results by condition for each task') +
  xlab('Meaning') +
  ylab('Response Probability') +
  ylim(0, 1)
```

```{r eval = F}
# The plot demonstrates that participants were more likely to produce the frequent suffix with a novel meaning when the frequent suffix had a high type frequency, but were less likely to produce the frequent suffix with a novel meaning when the frequent suffix had a high token frequency. Our results also demonstrate that this effect disappears when both forms are made accessible in the form-choice task. Further, the results also demonstrate that in comprehension, regardless of the condition or stem familiarity, the original meaning is chosen far more than the novel meaning for a given suffix.
```



We explore the results for each task in depth in the next sections.[^2]

[^2]: All data and code for the analyses can be found here: <https://github.com/znhoughton/Generalizability-Type-Token>.

### Production Task

<!--# Freq bar in original is choosing dan for big_pl or nem for dim_sg in the dan language and infreq bar is choosing dan for big_pl and nem for dim_sg in the nem language -->

In order to determine whether the effect of frequency on semantic extension differed between conditions, we ran a Bayesian linear mixed-effects model on the production data. The dependent variable was whether the participant produced the frequent suffix. The frequent suffix was coded as 1 if participants chose *dan* in the Dan language or chose *nem* in the Nem lan

We treatment coded condition such that the intercept was the type-token frequency condition. Thus, a larger intercept indicates that the frequent suffix was more likely to be produced than the infrequent suffix when it had a high type and token frequency. A larger coefficient estimate indicates that the frequent form was more likely to be produced than the infrequent form in that condition relative to the type-token frequency condition. We also included meaning and stem novelty as sum-coded variables. Meaning was a categorical variable with two levels: original and novel. An original meaning referred to big plural if the suffix was *dan* or diminutive singular if the suffix was *nem*. A meaning was novel if it was diminutive plural regardless of the suffix. stem novelty similarly was a categorical variable with two levels, familiar or novel. A familiar stem was one that participants saw in training while a novel stem was one that they did not see in training. We also included a random intercept for participant and random slopes for meaning by participant and stem novelty by participant. The syntax for our model is included below in @eq-prodmodelstem. The results are reported in @tbl-prodresultsstem and visualized in @fig-prodresultsstem.

$$
\text{frequent\_suffix} \sim \text{condition}*\text{meaning}*\text{stem} + (1 + \text{meaning} * \text{stem} | \text{participant})
$$ {#eq-prodmodelstem}

We find a meaningful effect for the intercept, suggesting that the frequent suffix is chosen more than the infrequent suffix when there is a high type and high token frequency. We also find a meaningful effect for novel stems, suggesting that in general the frequent suffix is produced more than the infrequent suffix when the stem is novel. Perhaps most interestingly, we find a meaningful interaction between the type frequency condition and novel stem, suggesting that in the type frequency condition, there is a preference for the frequent suffix over the infrequent suffix when the stem is novel but not when it is familiar. On the other hand, no such interaction effect is found in the token frequency condition. Further, we find a three-way interaction between type frequency condition, novel meaning, and novel stem, suggesting that learners are more likely to produce a frequent suffix in the type frequency condition when both the stem and meaning are novel.

Overall, our results suggest that in production, learners generally use the frequent suffix more regardless of meaning or stem familiarity if the suffix has both a high type and high token frequency. Additionally, when the frequent suffix has a higher type frequency than the infrequent suffix, learners are more likely to use it if the stem is novel, but not when the stem is familiar. Finally, in general there is no preference to use the frequent suffix more for novel meanings if the suffix has only a higher token frequency than the infrequent suffix.

```{r, echo = F, message = F}
#| label: tbl-prodresultsstem
#| tbl-cap: 'Results of the statistical models for the production task.'

prod_both_meanings_stems = bind_rows(fixefs_model_prod_both_stems, .id = "ID") %>%
  mutate(term = rep(c('Intercept (Type-Token Frequency)', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1))


prod_both_meanings_stems = prod_both_meanings_stems %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

prod_both_meanings_stems %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '20em')
```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-prodresultsstem
#| fig-cap: "Plot of the statistical model estimates for the production task. The x-axis indicates the condition (type frequency, token frequency, or type-token frequency). The y-axis corresponds to the proportion of responses with a frequent suffix. Blue points indicate that the meaning was novel while purple points corresponds to the original meaning. The facet indicates whether the stem was novel or familiar."
#| fig-height: 5
#| fig-width: 7


plot_prod_stem = plot_model(model_prod_both_stems, type = 'int',
                            axis.title = c('Condition', 'Frequency'),
                            axis.labels = "condition",
                            legend.title = 'Meaning',
                            title = "",
                            dot.size = 2,
                            line.size = 1,
                            colors = c('#298c8c', '#800074'))[[4]] 
plot_prod_stem[[11]]$linetype = 'Meaning'
plot_prod_stem[[11]]$shape = 'Meaning'

plot_prod_stem[[1]]$group_col = factor(plot_prod_stem[[1]]$group_col, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$group = factor(plot_prod_stem[[1]]$group, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$facet = factor(plot_prod_stem[[1]]$facet, levels = c('novel', 'familiar'), labels = c('Novel Stem', 'Familiar Stem'))

#plot_prod_stem[[1]]$x

plot_prod_stem +
  scale_x_continuous(breaks = c(1, 2, 3), 
                     labels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency')) +
  ylab('Proportion of Responses with Frequent Suffix') +
  scale_y_continuous(limits=c(0,1), breaks=seq(0, 1, by=0.25)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black', size = 10),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        axis.text.y = element_text(color = 'black', size = 10)) 

```

### Form Choice Task

In order to examine the effects of form-choice, we similarly ran a model analogous to the one we ran for the production task (@eq-prodmodelstem). In the context of the form-choice task, the dependent variable reflects whether participants chose the option with the frequent suffix or the one with the infrequent suffix. Analogous to the first production model, the independent variables were condition, meaning, and stem novelty. Once again, condition was treatment coded such that type-token frequency was the reference level and meaning and stem novelty were sum-coded. The random-effects structure was identical to that in @eq-prodmodelstem. The model results are presented in @tbl-2afcresultsstem and visualized in @fig-2afcresultsstem.

```{r, echo = F, message = F}
#| label: tbl-2afcresultsstem
#| tbl-cap: 'Results of the statistical models for the form-choice task.'

c2afc_both_stem = bind_rows(fixefs_model_2afc_both_stem, .id = "ID") %>%
  mutate(term = rep(c('Intercept (Type-Token Frequency)', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1))


c2afc_both_stem = c2afc_both_stem %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

c2afc_both_stem %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '20em')
```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-2afcresultsstem
#| fig-cap: "Plot of the statistical model estimates for the form-choice task. The x-axis indicates the condition (type frequency, token frequency, or type-token frequency). The y-axis corresponds to the proportion of responses with a frequent suffix. Blue points indicate that the meaning was novel while purple points corresponds to the original meaning. The facet indicates whether the stem was novel or familiar."
#| fig-height: 5
#| fig-width: 7

plot_prod_stem = plot_model(model_2afc_both_stem, type = 'int',
                            axis.title = c('Condition', 'Frequency'),
                            axis.labels = "condition",
                            legend.title = 'Meaning',
                            title = "",
                            dot.size = 2,
                            line.size = 1,
                            colors = c('#298c8c', '#800074'))[[4]] 
plot_prod_stem[[11]]$linetype = 'Meaning'
plot_prod_stem[[11]]$shape = 'Meaning'

plot_prod_stem[[1]]$group_col = factor(plot_prod_stem[[1]]$group_col, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$group = factor(plot_prod_stem[[1]]$group, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$facet = factor(plot_prod_stem[[1]]$facet, levels = c('novel', 'familiar'), labels = c('Novel Stem', 'Familiar Stem'))

#plot_prod_stem[[1]]$x

plot_prod_stem +
  scale_x_continuous(breaks = c(1, 2, 3), 
                     labels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency')) +
  scale_y_continuous(limits=c(0,1), breaks=seq(0, 1, by=0.25)) +
  ylab('Proportion of Responses with Frequent Suffix') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, color = 'black', size = 10),
        axis.title.x = element_text(size = 11),
        axis.title.y = element_text(size = 11),
        axis.text.y = element_text(color = 'black', size = 10)) 

```


In general we find no meaningful effects except for an interaction effect between type frequency and novel meaning, suggesting that there may be a preference for the frequent suffix when it has a higher type frequency than the infrequent suffix. However, the lack of any other meaningful effects suggests that when participants are presented with both possible options, for all conditions (except when there is a novel meaning in the type frequency condition), participants are equally likely to choose the frequent suffix as the infrequent suffix (i.e., token and type frequency don't seem to effect this).


### Comprehension Task

In order to examine the results for the comprehension task, similar to the previous tasks we ran a mixed-effects regression model. The dependent variable was whether the meaning that participants selected was novel or original. A positive estimate indicates that participants chose the novel meaning while a negative estimate indicates that participants were more likely to choose the original meaning. Unlike the other tasks, our independent variable was coded quite differently. Specifically, our conditions varied in what the token-to-type frequency was. For example, in the Type Frequency condition, while the frequent suffix occurs 12 times with 12 different types, the infrequent occurs 12 times in only 3 different types. Since this approach maximizes our power, rather than dividing the data based on whether the suffix was frequent or infrequent, we instead divided the data based on the relationship between the type and token frequency of the suffix, regardless of whether it was the frequent suffix or the infrequent suffix. More specifically, we divided the data up based on whether the token-to-type ratio for the form presented on a given trial was 12-12, 12-3, or 3-3. By comparing the 12-3 condition to the 12-12 and 3-3 conditions, we can see how an increase in type frequency and a decrease in token frequency affect participants' choice of the novel vs. original meaning. Thus, hereafter we refer to the 12-3 condition as the Control condition, the 12-12 condition as Higher Type and the 3-3 condition as Lower Token. 

We treatment coded condition (referred to as condition_numeric) such that the intercept was the Control condition. As a result, for each condition, a positive coefficient indicates that participants are more likely to choose a novel meaning (relative to the Control condition/intercept). The model syntax is included below in @eq-comprehensionmodel. Our results for the comprehension task are included in @tbl-comprehensionresults and visualized in @fig-comprehensionresults.

$$
\text{meaning} \sim \text{condition\_numeric} + (1 | \text{participant})
$$ {#eq-comprehensionmodel}

First, we find a meaningful effect for the Control condition (Intercept), suggesting that participants are more likely to select the original meaning when there is a high token-to-type ratio. Additionally, we find positive coefficient estimates for both Higher Type and Lower Token conditions suggesting that when an increase in type frequency or decrease in token frequency is present, participants are more likely to select the novel meaning than when there is a higher token-to-type ratio. These results suggest that it may not be type or token frequency independently that drives entrenchment, but rather the ratio between type and token frequency. That is, regardless of whether there is an increase in type frequency, or decrease in token frequency, if the token-to-type ratio is lower, participants are more likely to select the novel meaning.

```{r, echo = F, message = F}
#| label: tbl-comprehensionresults
#| tbl-cap: 'Results of the statistical model for the comprehension task.'
#| 

fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(term = rep(c('Intercept (Control)', 'Higher Type', 'Lower Token'), times = 1))


fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_model_comprehension_numeric %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

```{r, echo = F, message = F,  out.width = '100%', fig.align = 'center',}
#| label: fig-comprehensionresults
#| fig-cap: 'Plot of the model estimates for the comprehension task.'

plot_comprehension_no_stem = conditional_effects(model_comprehension_numeric)
plot(plot_comprehension_no_stem, plot = FALSE)[[1]] +
  theme_bw() +
  ylab('Probability of Novel Meaning') +
  xlab('Condition') +
  ylim(c(0,1)) +
  geom_errorbar(width = 0.1) +
  scale_x_discrete(labels = c("Control", "Higher Type", "Lower Token")) +
  scale_y_continuous(limits=c(0,1), breaks=seq(0, 1, by=0.25))


```

<!--# there's probably a note about storage here -->

Similar to the form-choice task, we also ran a model with stem familiarity as a fixed-effect. The dependent variable remained the same (whether participants chose the original or novel meaning), however in addition to condition, we also included stem novelty (familiar or novel) as a fixed-effect, along with its interaction with condition. A random intercept for participant was also included.

The results of the model are presented in @tbl-comprehensionresultsstem. Our results show no effect of stem familiarity on comprehension.

```{r, echo = F, message = F}
#| label: tbl-comprehensionresultsstem
#| tbl-cap: 'Results of the statistical model for the comprehension task with stem as a fixed-effect.'
#| 

fixefs_model_comprehension_numeric_stem = fixefs_model_comprehension_numeric_stem %>%
  mutate(term = rep(c('Intercept', '12-12', '3-3', 'Novel Stem', '12-12:Novel Stem', '3-3:Novel Stem'), times = 1))


fixefs_model_comprehension_numeric_stem = fixefs_model_comprehension_numeric_stem %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_model_comprehension_numeric_stem %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')


```

Overall, the comprehension results demonstrate that participants are more likely to select the original meaning when there is a high token-to-type ratio. However, when the token-to-type ratio is equivalent (either 12-12 or 3-3) then participants are more likely to select the novel meaning than when there is a high token-to-type ratio. One caveat is that participants still prefer the original meaning over the novel meaning in general (this is evident because adding the upper CI estimate to the intercept for both conditions still results in a negative estimate). In other words, while decreasing the token-to-type ratio decreases the preference for the original meaning, it does not eliminate it.

## Rescorla-Wagner Model

In this section we examine whether the Rescorla-Wagner model [@rescorla1972theorypavlovianconditioning], an associative learning model, can accurately predict our data or not.

The Rescorla-Wagner model is an associative learning model that has gained a great deal of popularity. Despite its simplicity, it has been shown to account for many phenomena in language learning and processing [@ellisSelectiveAttentionTransfer2006; @olejarczukDistributionalLearningErrordriven2018; @ramscarChildrenValueInformativity2013; @baayen2012demythologizingwordfrequency].

Specifically, the Rescorla-Wagner model is a two-layer feedforward neural network. It takes as its input a set of cues and predicts an outcome [@rescorla1972theorypavlovianconditioning; @kapatsinskiLearningFastAvoiding2021]. The Rescorla-Wagner model is defined below in @eq-rwmodel and @eq-rwmodelabsent, which differ in whether the predicted outcome is present or not. The model learns the weight of an association between a present cue and a present outcome. In the equation, $\alpha_C$ denotes the salience of the cue, $\beta^P_O$ denotes the salience of an outcome when it is present, and $\beta^A_O$ denotes the salience of an outcome when it is absent [@kapatsinskiLearningFastAvoiding2021]. $\lambda_{max}$ is the learning rate.

$$
\Delta V_{C\rightarrow O } = \alpha_C\beta^P_O(\lambda_{max}-\alpha_O)
$$ {#eq-rwmodel}

$$
\Delta V_{C\rightarrow O } = \alpha_C\beta^A_O(\lambda_{min}-\alpha_O)
$$ {#eq-rwmodelabsent}

While the Rescorla-Wagner model is often used with a linear activation function, in the present simulations we use a logistic activation instead (@eq-rwlogistic) . The motivation behind this decision is that the Rescorla-Wagner model with a logistic activation function is more sensitive to type frequency than the Rescorla-Wagner model with a linear activation function [@caballero2022howagglutinativesearching]. Further, as @kapatsinskiLearningFastAvoiding2021 demonstrates, using a logistic activation function mitigates spurious excitement, a side-effect of using the linear activation function whereby an absent cue and an absent outcome become associated with eachother. 

$$
\alpha_O = logit^{-1}(\sum_cV_{C\rightarrow O})
$$ {#eq-rwlogistic}

Given the Rescorla-Wagner model's success on a wide variety of linguistic phenomena, it's possible that it is also able to predict the results we see in this paper. Thus in this section we simulate the Rescorla-Wagner predictions for our production data in each exposure condition and compare it to the human data.

For the simulations in this section, $\alpha$ and $\beta$ were set to 0.1, $\lambda_{min}$ was set to 0, and $\lambda_{max}$ was set to 2. Our simulation process was as follows: First, for each of the different exposure conditions, we obtained a matrix of predicted cue-outcome associations using the Rescorla-Wagner model. This was done by feeding the model each trial of the exposure phase in a random order. The cues were the stem identity along with the meaning (e.g., bal_dim_pl) and the outcome was the suffix that the stem occurred with on that trial (i.e., *dan*, *nem*, *sil*, or *shoon*). The model then learned associations between each cue and each outcome.

Next, for each of the items in the production task, we summed the associations for each cue-outcome present in that trial. For example, given the item *baldan* and the meaning big plural, we consulted the matrix of cue-outcome associations for *bal* and *dan*, big and *dan*, and plural and *dan*. We then summed these to get the predicted associations for *bal* with the big.pl meaning and *dan*. This resulted in a matrix that included the model's learned association strength for any given cue with each of the four suffixes.

Following this, in order to compare the simulations with the human data, we calculated association strengths depending on whether the meaning was original or familiar. For the original meanings, we subtracted the activation strengths between the original meaning and the appropriate suffix. Specifically, this is the difference in activation strength between the meaning cue big plural and the outcome *dan*, and the meaning cue diminutive singular and the outcome *nem*. If the language was dan (i.e., *dan* was the frequent suffix), then the activation strength between diminutive singular and *nem* was subtracted from the activation strength between big plural and *dan*. If the language was nem (i.e., *nem* was the frequent suffix), then the subtraction was in the opposite direction. To calculate the association strengths for novel meanings, if the frequent suffix was *dan* then the activation strength between diminutive plural and *nem* was subtracted from the activation strength between diminutive plural and *dan*. If the frequent suffix was *nem* then the opposite was done. This results in a single value for each trial that is the model's predicted activation of the frequent suffix relative to the infrequent suffix.

In order to test our model, we evaluated whether the model is a good predictor of the human data. Thus, we ran two Bayesian logistic regression models. In each model, the dependent variable was whether the human learner, for a given trial, selected the frequent suffix or the infrequent suffix. The first model was a simple model that modeled the human data as a function of the Rescorla-Wagner activation strength (referred to as `freq_minus_infreq` in the model syntax), with random intercepts for participant and stem identity (@eq-rwmodel1). In our second model, we calculated separately the stem activation and the meaning activation (using the RW model). The motivation behind this is that the Rescorla-Wagner model is agnostic to whether the cue is a stem or a meaning (because it originally has no way of knowing whether a given cue is part of the stem or the meaning), but participants may show varying sensitivity to one over the other. Modeling it in this manner allows the statistical model to fit different slopes for the stem activations and the meaning activations. We also included fixed-effect for condition as well as an interaction effect between condition and the stem activations, and an interaction between the meaning activations and condition (@eq-rwmodel3).


$$
\text{frequency} \sim \text{freq\_minus\_infreq} + (1 | \text{participant}) + (1|\text{resp\_stem})
$$ {#eq-rwmodel1}

```{r eval = F}
# $$
#\begin{aligned}
#\text{frequency} & \sim \text{freq\_minus\_infreq\_stem} * \text{freq\_minus\_infreq\_meaning} \\ & + (1 | \text{participant}) + (1|\text{resp\_stem})
#\end{aligned}
#$$ {#eq-rwmodel2}

# $$
# \begin{aligned}
# \text{frequency} &\sim  \text{condition}*\text{meaning}*\text{stem\_condition} + \text{freq\_minus\_infreq} \\ & + (1  | \text{participant}) + (1 | \text{resp\_stem})
# \end{aligned}
# $$ {#eq-rwmodel4}
# 
# $$
# \begin{aligned}
# \text{frequency} &\sim  \text{condition}*\text{meaning}*\text{stem\_condition} \\ & + \text{freq\_minus\_infreq\_stem} * \text{freq\_minus\_infreq\_meaning} * \text{condition} \\ & + (1  | \text{participant}) + (1 | \text{resp\_stem})
# \end{aligned}
# $$ {#eq-rwmodel5}
```



$$
\begin{aligned}
\text{frequency} & \sim \text{freq\_minus\_infreq\_stem} + \text{freq\_minus\_infreq\_meaning} + \text{condition} \\ & + \text{freq\_minus\_infreq\_stem:condition} + \text{freq\_minus\_infreq\_meaning:condition} \\ & + (1 | \text{participant}) + (1|\text{resp\_stem})
\end{aligned}
$$ {#eq-rwmodel3}


The results of the models are presented in @tbl-rwsim1 and @tbl-rwsim3.

```{r, echo = F, message = F}
#| label: tbl-rwsim1
#| tbl-cap: 'Results of the statistical model with only the RW predictions (@eq-rwmodel1).'


fixefs_human_rw_preds = as.data.frame(fixef(model_human_rw_preds)) %>%
  mutate(term = rep(c('Intercept', 'RW Predictions'), times = 1))


fixefs_human_rw_preds = fixefs_human_rw_preds %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5), as.numeric))


fixefs_human_rw_preds %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')

```

```{r, echo = F, message = F, eval = F}
#| label: tbl-rwsim2 
#| tbl-cap: 'Results of the statistical analysis containing the predictions from the Rescorla-Wagner logistic model separated into stem activations and meaning activations. The results demonstrate that the human data is predicted well by both the suffix activations and the stem activations (eq-rwmodel2).'

fixefs_model_rw_preds_stem_suffix_vary = bind_rows(fixefs_model_rw_preds_stem_suffix_vary, .id = "ID") %>%
  mutate(term = rep(c('Intercept', 'Stem Activations', 'Meaning Activations'), times = 1))


fixefs_model_rw_preds_stem_suffix_vary = fixefs_model_rw_preds_stem_suffix_vary %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

fixefs_model_rw_preds_stem_suffix_vary %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')

```
```{r, echo = F, message = F}
#| label: tbl-rwsim3 
#| tbl-cap: 'Results of the statistical analysis containing the separated stem and meaning activations and condition as a fixed-effect (@eq-rwmodel3).'

fixefs_model_rw_preds_stem_suffix_vary_by_condition = bind_rows(fixefs_model_rw_preds_stem_suffix_vary_by_condition, .id = "ID") %>%
  mutate(term = rep(c('Intercept', 'Stem Activations', 'Meaning Activations', 'Type Frequency', 'Token Frequency', 'Stem Activations:Type Frequency', 'Stem Activations:Token Frequency', 'Meaning Activations:Type Frequency', 'Meaning Activations:Token Frequency'), times = 1))

fixefs_model_rw_preds_stem_suffix_vary_by_condition = fixefs_model_rw_preds_stem_suffix_vary_by_condition %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

fixefs_model_rw_preds_stem_suffix_vary_by_condition %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')

```

```{r, echo = F, message = F, eval = F}
#| label: tbl-rwsim4
#| tbl-cap: 'Results of the statistical model with the RW predictions alongside the original predictors (@eq-rwmodel4).'


percent_greater_zero = data.frame(fixef(model_prod_both_stem_rw_simple, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_model_human_and_rw = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'freq_minus_infreq', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1')))


fixefs_human_and_rw_predictors = as.data.frame(fixef(model_prod_both_stem_rw_simple)) %>%
    mutate(percent_greater_zero = percent_greater_zero_model_human_and_rw$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


fixefs_human_and_rw_predictors = fixefs_human_and_rw_predictors %>%
  mutate(term = rep(c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Frequent - Infrequent (RW)', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1))

fixefs_human_and_rw_predictors = fixefs_human_and_rw_predictors %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_human_and_rw_predictors %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

```{r, echo = F, message = F, eval = F}
#| label: tbl-rwsim5
#| tbl-cap: 'Results of the statistical model with the RW predictions separated into stem and meaning activations alongside the predictors from the human model (@eq-rwmodel5).'


percent_greater_zero = data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition_original_preds, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


percent_greater_zero_model_rw_preds_stem_suffix_vary_by_condition_original_preds = percent_greater_zero %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'stem_condition1', 'freq_minus_infreq_stem', 'infreq_minus_infreq_suffix', 'condition1.meaning1', 'condition2.meaning1', 'condition1.stem_condition1', 'condition2.stem_condition1', 'meaning1.stem_condition1', 'condition1.freq_minus_infreq_stem', 'condition2.freq_minus_infreq_stem', 'condition1.freq_minus_infreq_suffix', 'condition2.freq_minus_infreq_suffix',  'condition1.meaning1.stem_condition1', 'condition2.meaning1.stem_condition1')))



fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds = as.data.frame(fixef(model_prod_rw_stem_suffix_vary_by_condition_original_preds)) %>%
    mutate(percent_greater_zero = percent_greater_zero_model_rw_preds_stem_suffix_vary_by_condition_original_preds$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds = fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds %>%
  mutate(term = rep(c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Stem Activations', 'Meaning Activations', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Stem Activations', 'Token Frequency:Stem Activations', 'Type Frequency:Suffix Activations', 'Token Frequency:Suffix Activations', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1)) #need to fix this

fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds = fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_model_rw_preds_stem_suffix_vary_by_condition_original_preds %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

Overall we find that the general RW activations for both stem and meaning are able to account for the human data. Further, we find that when token frequency is high, meaning activations are a better predictor (evident by the interaction effect between Meaning Activations and Token Frequency in @tbl-rwsim3).

In order to compare which model best fits the data, we performed leave-one-out cross-validation using the R package `loo` [@loopackage]. Leave-one-out cross-validation refits the model for each observation in the dataset, leaving out that observation.[^3] For each data point, the expected log predictive density is calculated. Expected log predictive density is the log-likelihood of the held-out observation given the model's parameters (trained without the observation). Each model receives a single expected log predictive density value by summing the log-likelihood of each observation using leave-one-out cross-validation. A higher value indicates the model performs better than the other models. Thus, we compared the expected log predictive density value for each of these three models, along with the model of the human data that does not contain the Rescorla-Wagner predictions.

[^3]: More accurately, Bayesian models are computationally expensive to fit many times, so this is approximated using Pareto-smoothed importance sampling instead.

We performed leave-one-out cross validation for each of the two models with the RW predictors as well as the model with just the human predictors. The results of our leave-one-out cross-validation are included in @tbl-loocv. When using leave-one-out cross-validation, a model can be considered as outperforming another model if the difference in elpd is greater than the standard error of the elpd. In the case of our models, because all of the elpd differences are less than the standard error of the elpd, it is difficult to draw conclusions about which model fits the data best.

```{r, echo = F, message = F}
#| label: tbl-loocv 
#| tbl-cap: 'Results of our leave-one-out cross-validation.'
terms_of_interest = c('Only RW Activation', 'Stem and Meaning Activations by Condition', 'Original Statistical Predictors')
loo_co_results = loo_co_results %>%
  select(elpd_diff, se_diff, elpd_loo, se_elpd_loo, term) %>%
  filter(term %in% terms_of_interest)
  

loo_co_results %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '22em')
```


```{r eval = F}
#@fig-rwvshumanpreds provides a visualization of the model predictions compared to the human results. The visualization demonstrates high agreement between the predictions and human results for the type frequency condition and the type-token frequency condition, but falls short in predicting the human results for the token frequency condition. More specifically, the model predicts an effect of stem familiarity for the token frequency condition that does not appear in the human data. One possible explanation for this difference is that humans be sensitive to the fact that stem identity is a useful predictor of the meaning in the type frequency condition but not in the token frequency condition. Specifically, in the type frequency condition, certain stems appeared only with one suffix, however in the token frequency condition all the stems occurred with all of the suffixes. This may lead to learners paying attention to the stem identity in the type frequency condition but not in the token frequency condition. On the other hand, the model learns to associate all of the cues with the present outcomes irregardless of whether that cue occurs with more than one outcome.
```

<!--# could test this with eye-tracking if we really wanted to, do people fixate longer on stems in the type frequency condition compared to the token frequency condition? -->

### Additional Simulations

There is evidence that humans learn associations for configural cues that their parts do not have [@kapatsinskiTestingTheoriesLinguistic2009]. For example, in English syllables, rimes are treated as a single constituent (@fig-syllstructure) and @kapatsinskiTestingTheoriesLinguistic2009 demonstrated that native English speakers can learn to associate rimes with an outcome even in cases when the onset and nucleus are associated with different outcomes.

```{r echo = F, fig.align = 'center', warning = F, message = F, out.width = '50%'}
#| label: fig-syllstructure
#| fig-cap: "A visualization of English syllable structure."
#| fig-align: center

knitr::include_graphics("English syllable structure.pdf")
```

There is also evidence that adult native English speakers show greater reliance on semantic cues than phonological cues in production [@culbertson2018childrenprivilegephonological]. For example, @culbertson2018childrenprivilegephonological demonstrated that in learning an artificial noun class adults rely more on semantic cues than phonological cues (whereas interestingly children rely more on phonological cues than semantic cues).

Thus, in order to further examine the differences between the human and model predictions we made two modifications to the simulations. First, instead of modeling the suffix meanings (diminutive plural, diminutive singular, big plural, big singular) as two separate cues, we also included a configural cue (analogous to an interaction effect in linear regression) that could be associated with the outcome. This configural cue was simply a combination of the two meanings. For example, given the meaning "big plural", the cues comprised "big", "plural", and "big.plural". Similar to the previous simulations, the model was presented with these cues paired with one of the four suffixes. The model was trained on the same data that the humans were trained on.

Second, in addition to configural cues, we also increased the alpha value for semantic cues relative to the phonological cues. This results in semantic cues being more associable with outcomes than phonological cues. Specifically, alpha was 0.05 for phonological cues and 0.25 for semantic cues. All other variables remained unchanged.

Analogous to the previous simulations, we then calculated the model's predicted activation for the frequent suffix. If the meaning was novel, this was simply the difference between the activation strength for the frequent suffix and the activation strength for the infrequent suffix. If the meaning was original, then it was the difference between activation strength of big plural with *dan* and the activation strength of diminutive singular and *nem* if *dan* was the frequent suffix and vice versa if *nem* was the frequent suffix.

In order to evaluate this simulation, we ran a Bayesian logistic regression model with the human response as the dependent variable (1 if they chose the frequent suffix and 0 if they chose the infrequent suffix) and the model's prediction as the independent variable with random intercepts for participant and stem. We ran four models, one with configural cues and modified semantic salience, one with configural cues, increased semantic salience, and the original statistical predictors ($condition*meaning*stem\_condition$), one with only configural cues (no modified semantic salience) and one with only modified semantic salience (no configural cues). We then compared these with the model with only statistical predictors, and the two models from the previous simulations (@eq-rwmodel1 and @eq-rwmodel3).

We then compared these models using leave-one-out-cross-validation (@tbl-loocv_addsims).

```{r, echo = F, message = F}
#| label: tbl-loocv_addsims 
#| tbl-cap: "Results of the leave-one-out cross-validation for the additional simulation. 'Modified RW' corresponds to the model with configural cues and increased saliency for semantic cues. 'RW Model' refers to the RW logistic model. 'Statistical Predictors' refers to the original statistical analysis that contains no RW model predictions."


loo_co_results_add = loo_co_results_add %>%
  dplyr::select(elpd_diff, se_diff, elpd_loo, se_elpd_loo, term) 


loo_co_results_add %>%
  dplyr::select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '22em')

```

The results of leave-one-out cross-validation suggest that the models that include configural cues or increased sensitivity to semantic cues outperform all the original models as well as the model with only statistical predictors. Among those four, any of the models that include semantic predictors perform better than the model that includes only configural cues. In order to visualize this, we also included a side-by-side plot of the human data with the RW activations from the model that included both modified semantic salience and configural cues (@fig-rwvshumanpreds). For comparison, we also plotted the activations from the model without modified semantic salience or configural cues.

<!--# to add: in-depth explanation of the configural cue and semantic  -->

```{r, echo = F, message = F, eval = F}
#| label: tbl-rwconfsem 
#| tbl-cap: 'Results of the statistical analysis containing the predictions from the Rescorla-Wagner logistic model with configural cues and increased saliency for semantic cues.'

fixefs_model_model_human_conf_sem_preds = fixefs_model_model_human_conf_sem_preds %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric)) %>%
  mutate(term = c('Intercept', 'RW Model Predictions'))


fixefs_model_model_human_conf_sem_preds %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 11, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')

```

```{r, echo = F, out.width = '90%', fig.align = 'center', warning = F, message = F}
#| label: fig-rwvshumanpreds
#| fig-cap: "Plot of the original RW suffix activations and the modified RW activations (configural cues plus modified salience of semantic cues) versus the human results. The y-axis is the probability of producing the frequent meaning. The top facet is original meanings and the bottom axis is novel meanings. Along the x-axis, 'Modified Sim' corresponds to the RW simulations with configural cues and increased semantic saliency. 'RW Logistic' corresponds to our original suffix activations. 'Human' corresponds to the original human data. The visualization shows that the original RW suffix activations seem to fall short in predicting the human data for the token frequency condition."
#| fig-height: 6
#| fig-width: 7

config_mod_sem = read_csv('../Data/rw_plot_configural_mod_alpha.csv') %>%
  mutate(model_type = 'Modified Sim', Prob = arm::invlogit(mean_freq_diff))

fixed_effects_df = read_csv('../Data/fixed_effects_df_all.csv') %>%
  filter(Task == 'prod') %>%
  mutate(model_type = 'Human') %>%
  rename('meaning' = 'Meaning', 'condition' = 'Condition', 'mean_freq_diff' = 'Estimate', 'prod_condition' = 'Stem') %>%
  mutate(condition = case_when(
    condition == 'cond1' ~ 1,
    condition == 'cond2' ~ 2,
    condition == 'cond3' ~ 3
  )) %>%
  dplyr::select('meaning', 'prod_condition', 'condition', 'Prob', 'model_type')

no_config_cues = read_csv('../Data/data_all_conds_logistic.csv') %>%
  mutate(model_type = 'RW Logistic', Prob = arm::invlogit(mean_freq_diff))
  

all_data = config_mod_sem %>%
  full_join(fixed_effects_df) %>%
  full_join(no_config_cues)

#all_data = read_csv('../Data/model_preds_and_human_data.csv')

all_data$Meaning = all_data$meaning
all_data$Condition = all_data$condition
all_data$Stem = all_data$prod_condition


all_data = all_data %>%
  mutate(across(where(is.character), str_to_title)) %>%
  mutate(Condition = case_when(
    Condition == 1 ~ 'Type Frequency',
    Condition == 2 ~ 'Token Frequency', 
    Condition == 3 ~ 'Type-Token Frequency'
  ))

all_data$Condition = factor(all_data$Condition, levels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency'))

observed_vs_model_plot = ggplot(data = all_data, aes(x = model_type, y = Prob, fill = Stem)) +
  geom_col(position = "dodge") +
  xlab('Human Data vs Model Predictions') +
  ylab('Probability of the Frequent Meaning') +  
  scale_fill_manual(values = c("Familiar" = "#66c2a5", "Novel" = "#8da0cb")) +
  ylim(c(0,1)) +
  facet_grid(Meaning~Condition) +
  theme_bw() +
  theme(
  axis.text.x = element_text(angle = 45, hjust = 1)
)

observed_vs_model_plot

#ggarrange(observed_estimates_plot, rw_plot, nrow = 2, common.legend = T)
```

Overall, the results of our simulations suggest that an error-driven associative learning model, specifically the Rescorla-Wagner model, fits the human data well. Further, a model with configural cues and an increased salience of semantic cues fits the human data better than even the original statistical predictors. This parallels previous findings in demonstrating that humans learn associations for configural cues and that humans are more sensitive to semantic cues than phonological cues.

## Discussion

Our results suggest that in production, 

Our results suggest that in production, having a high type and high token frequency together lead to a preference for the frequent suffix, and this preference is even stronger if the stem is novel. On the other hand, when there is a high type frequency but not a high token frequency, there is only a preference for the frequent form when the stem and meaning are both novel. Finally, for token frequency there is only a preference for the frequent form when the meaning is original. Overall, these results suggest that type frequency and token frequency both play a role in semantic extension, with a high type frequency encouraging semantic extension while a high token frequency encourages using the suffix to communicate the original meaning. Similar to @harmonPuttingOldTools2017, these effects are mitigated when both forms are brought into memory.

On the other hand, a similar pattern emerges. When there is a high token-to-type ratio, the original meaning is preferred more. However, when there are as many types as there are tokens, there is an increased preference for the novel meaning relative to when there is a high token-to-type ratio.

Our results also demonstrate that the Rescorla-Wagner model with a logistic activation function is able to capture the human data. Further, the model matches the human data best when it includes configural cues and an increased salience for semantic cues relative to phonological cues. 

\newpage
