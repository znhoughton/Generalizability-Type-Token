---
title: "generalizability-writeup"
format: pdf

header-includes:
- \usepackage{amsmath}

editor: visual
bibliography: references.bib
---

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(knitr)
library(kableExtra)
library(brms)
library(sjPlot)

fixed_effects_df_all = read_csv('../Data/fixed_effects_df_all.csv')
subject_effects = read_csv('../Data/subject_effects.csv')

fixed_effects_df_all$Task = factor(fixed_effects_df_all$Task, levels = c('prod', '2afc', 'comprehension'))

subject_effects$Task = factor(subject_effects$Task, levels = c('prod', '2afc', 'comprehension'))

condition_labels = c("cond1" = "Type Frequency", 
                      "cond2" = "Token Frequency", 
                      "cond3" = "Type-Token Frequency")
task_labels = c("prod" = "Production",
                "2afc" = "Form Choice",
                "comprehension" = "Comprehension")

fixed_effects_df_all$Meaning = factor(fixed_effects_df_all$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

subject_effects$Meaning = factor(subject_effects$Meaning, 
                                  levels = c("original", "novel"), 
                                  labels = c("Original", "Novel"))

fixed_effects_df_all$Frequency = factor(fixed_effects_df_all$Frequency, 
                                  levels = c("freq", "infreq"), 
                                  labels = c("Frequent", "Infrequent"))

subject_effects$Frequency = factor(subject_effects$Frequency, 
                                  levels = c("freq", "infreq"), 
                                  labels = c("Frequent", "Infrequent"))


# model_prod_original = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_original,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_original')
# 
# model_prod_novel = brm(frequency ~ condition + (1 | participant) + (condition | resp_stem),
#                                        data = data_prod_novel,
#                                        family = bernoulli(link = 'logit'),
#                                        iter = 14000, 
#                                        warmup = 7000,
#                                        chains = 4,
#                                        cores = 4,
#                                        prior = priors_m1,
#                                        control = list(adapt_delta = 0.99),
#                                        file = '../Data/model_prod_novel')



model_prod_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both')



percent_greater_zero_both = data.frame(fixef(model_prod_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_prod_both = as.data.frame(fixef(model_prod_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


model_prod_both_stems = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_prod_both_stem')



percent_greater_zero_both_stems = data.frame(fixef(model_prod_both_stems, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both_stems = percent_greater_zero_both_stems %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_prod_both_stems = as.data.frame(fixef(model_prod_both_stems)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both_stems$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)


rownames(fixefs_model_prod_both_stems) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem')


model_2afc_both = brm(frequency ~ condition*meaning + (1 + meaning | participant),
                                       data = data_prod_both,
                                       family = bernoulli(link = 'logit'),
                                       iter = 14000, 
                                       warmup = 7000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       control = list(adapt_delta = 0.99),
                                       file = '../Data/model_2afc_both')





percent_greater_zero_both = data.frame(fixef(model_2afc_both, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)

percent_greater_zero_both = percent_greater_zero_both %>%
  arrange(match(beta_coefficient, c('Intercept', 'condition1', 'condition2', 'meaning1', 'condition1:meaning1', 'condition2:meaning1')))


fixefs_model_2afc_both = as.data.frame(fixef(model_2afc_both)) %>%
    mutate(percent_greater_zero = percent_greater_zero_both$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
    mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)



rownames(fixefs_model_2afc_both) = c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel')





model_comprehension_numeric = brm(meaning ~ condition_numeric + (1 | participant),
                                       data = data_analysis_comprehension,
                                       family = bernoulli(link = 'logit'),
                                       iter = 10000, 
                                       warmup = 5000,
                                       chains = 4,
                                       cores = 4,
                                       prior = priors_m1,
                                       #control = list(adapt_delta = 0.99),
                                       file = '../Data/model_comprehension_numeric')

percent_greater_zero_comprehension_numeric = data.frame(fixef(model_comprehension_numeric, summary = F)) %>%
  pivot_longer(cols = everything(), names_to = 'beta_coefficient', values_to = 'estimate') %>%
  group_by(beta_coefficient) %>%
  summarize((sum(estimate > 0) / length(estimate)) * 100)


fixefs_model_comprehension_numeric = as.data.frame(fixef(model_comprehension_numeric)) %>%
    mutate(percent_greater_zero = percent_greater_zero_comprehension_numeric$`(sum(estimate > 0)/length(estimate)) * 100`) %>%
  mutate_if(is.numeric,
            formatC,
            format = 'f', 
            digits = 3) %>%
    rename('% Samples > 0' = `percent_greater_zero`)

rownames(fixefs_model_comprehension_numeric) = c('Intercept', '12-12', '3-3')

```

# Methods

Following @harmonPuttingOldTools2017, two artificial languages were used: Dan and Nem [See @fig-fig1]. In each language, the same four suffixes were used: -*sil~PL~*, *-dan~PL~*, *-nem~DIM~*, and -*shoon~DIM~*. Notably, in our language *-dan* and *-sil* overlap in meaning (they both occur in plural contexts) as do -*nem* and *-shoon* (which both occur in diminutive contexts).

![A description of the suffixes in our artificial languages. The thicker lines denote the more frequent form in each language. The Dan language is on the left, the Nem language is on the right.](fig1.jpg){#fig-fig1}

During the exposure phase, the suffixes were paired with an image. The suffixes -*sil~PL~* and *-dan~PL~* were always paired with a picture of multiple large pictures. On the other hand, the suffixes *-nem~DIM~* and -*shoon~DIM~* were always paired with a picture of a single small creature. The design of the stimuli result in participants being able to learn that -*sil* and *-dan* are either simply plural or simply non-diminutive. Similarly, *-nem* and *-shoon* can be learned as either simply singular or simply diminutive. Thus the test phase tested this.

Our Experiment comprised of three different conditions (see @tbl-conditionslist), one in which the type frequency of the frequent language's suffix was manipulated (Type Frequency), one in which the token frequency was manipulated (Token Frequency), and one in which both were manipulated (Type-Token Frequency).

```{r, echo = F, message = F}
#| label: tbl-conditionslist
#| tbl-cap: 'Description of each of our conditions. Note that in Condition 1, there are an equal number of tokens between the frequent and infrequent items, however there are a greater number of types in the frequent items. In Condition 2, the opposite is true: the frequent items occur more, but in the same number of types as the infrequent items. Finally, in Condition 3, the frequent items occur both a greater number of times and in a greater number of different contexts.'

conditionslist = data.frame('Frequent Token' = c(12, 12, 12),
                            'Frequent Type' = c(12, 3, 12),
                            'Infrequent Token' = c(12, 3, 3),
                            'Infrequent Type' = c(3, 3, 3))

conditionslist = conditionslist %>%
  mutate(term = rep(c('Type Frequency', 'Token Frequency', 'Type-Token Frequency'), times = 1))

conditionslist %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 10, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '12em')
```

## Procedure

Each participant was randomly assigned one of the conditions. In each condition, participants were first presented with an exposure phase. After the exposure phase, participants took part in a production task, a form choice task, and a comprehension task. We describe each of these below.

### Exposure Phase

Following @harmonPuttingOldTools2017, each exposure trial consisted of the presentation of a picture on the computer screen which was subsequently followed with a written label for the image as well as an audio version of that label. Specifically, the image first appeared on the screen and then 1.25 seconds later was followed by both the label of the creatures on the screen as well as the audio for that creature. Participants were instructed to type the name of the creature and press enter. Participants had 4 seconds after presentation of the name of the creature and were given feedback if they typed the wrong name in.

### Production Task

After the exposure phase, participants were presented with a production task. In this task, participants were presented with images and told to produce a label for the image. Specifically, initially the unafixxed form appeared on the screen along with the corresponding image. 2 seconds later, the four different possible images of that creature appeared (a singular big creature, multiple big creatures, a single small creature, and multiple small creatures). Three of these images disappeared after 1.25 seconds, leaving a single image for the participant to produce a label for. Participants had 10 seconds to respond. In the production task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

### Form Choice Task

After the production task, participants were presented with a form choice task. In this task, participants were presented first with the base, unaffixed form and the corresponding image. 2 seconds later four images flashed on the screen, remained on the screen for 1.25 seconds, and disappeared leaving a single image. Along with the single image, participants were also presented with two possible labels for that image, one label on the bottom right of the screen and one label on the bottom left of the screen. Participants were given four seconds to press either the left arrow or the right arrow to choose the corresponding label. The labels were counterbalanced with respect to which side of the screen they appeared on. In the form choice task, some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training). The goal of this task was to assess whether type and/or token frequency influence the form choice when accessibility differences between frequent and rare forms have been attenuated.

### Comprehension Task

Finally, participants were presented with a comprehension task. In this task, participants were first given the label and corresponding audio for a given creature. After 0.25 seconds, four images appeared on the screen and participants had 4 seconds to choose one of the images. Similar to the before-mentioned tasks, in the comprehension task some of the stems were familiar (i.e., seen in training) and others were novel (i.e., not seen in training).

# Analyses and Results

## Production Task

In order to examine the effect of type/token frequency on semantic extension, we examined instances where the image was diminutive plural.

```{r, echo = F, out.width = '80%', fig.align = 'center', warning = F, message = F}
#| label: fig-fullresults
#| fig-cap: "Plot of our results. Points indicate individual subject values."


ggplot(fixed_effects_df_all, aes(x = Meaning, y = Prob, fill = Frequency)) +
  geom_col(position = position_dodge(width = 0.9)) + 
  geom_errorbar(aes(ymax = Upper, ymin = Lower), width = 0.2, position = position_dodge(width = 0.9)) +
  geom_point(data = subject_effects, 
             aes(x = Meaning, y = subj_values, fill = Frequency, color = Frequency),  # Map fill and color
             shape = 21,   # Hollow circle with fill and outline
             position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.9), 
             size = 2, stroke = 0.7, alpha = 0.1,  # Adjust size, border thickness, and transparency
             show.legend = FALSE) +  # Hides points from legend
  scale_fill_manual(values = c("Frequent" = "#66c2a5", "Infrequent" = "#8da0cb")) +  # Bar & point fill colors
  scale_color_manual(values = c("Frequent" = "#1b7837", "Infrequent" = "#2b8cbe")) +  # Darker border colors for points
  facet_grid(Task ~ Condition, labeller = labeller(Condition = condition_labels, Task = task_labels)) + 
  theme_bw() +
  ggtitle('Results by condition for each task') +
  xlab('Condition') +
  ylab('Response Probability') +
  ylim(0, 1)
```

<!--# Freq bar in original is choosing dan for big_pl or nem for dim_sg in the dan language and infreq bar is choosing dan for big_pl and nem for dim_sg in the nem language -->

In order to determine whether the effect of frequency on semantic extension differed between conditions, we ran a Bayesian linear mixed-effects model on the production data. The dependent variable was whether the participant produced the frequent suffix (*dan* in the *dan* language, *nem* in the *nem* language). A larger estimate indicates that the frequent suffix was more likely to be produced than the infrequent suffix. The independent variables were condition, meaning, and stem (either familiar stem or novel stem). Condition was treatment coded such that the intercept was the high type and high token frequency condition. Meaning was sum coded. We also included a random intercept for participant and a random slope for meaning by participant. The equation for our model is included below in @eq-prodmodels.

$$
\text{frequent\_suffix} \sim \text{condition}*\text{meaning} + (1 + \text{meaning} | \text{participant})
$$ {#eq-prodmodels}

The results are below in @tbl-prodresults. The results suggest that in general the frequent suffix is used more than the infrequent suffix for all conditions. Notably however, we find no meaningful interaction effect for type frequency and novel meaning, suggesting that the effect of meaning is similar for both conditions. We do, however, find an interaction effect between token frequency and novel meaning, suggesting that the frequent suffix is used less for the novel meaning in the token frequency condition. This suggests that there is semantic extension when there is high type and high token frequency as well as the case where there is high type-frequency and low token-frequency, but in the case where there is high token-frequency with a low type-frequency, we see entrenchment in production.

```{r, echo = F, message = F}
#| label: tbl-prodresults
#| tbl-cap: 'Results of the statistical models for the production task.'

prod_both_meanings = bind_rows(fixefs_model_prod_both, .id = "ID") %>%
  mutate(term = rep(c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel'), times = 1))


prod_both_meanings = prod_both_meanings %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

prod_both_meanings %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 9, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '14em')
```

In order to follow up on this, we ran an additional model that included whether the stem was familiar (occurred in training) or novel (did not occur in training). The results are reported in @tbl-prodresultsstem and visualized in @fig-prodresultsstem.

Our follow-up analysis suggests that type-frequency leads to entrenchment with stems that learners have encountered before. However, for novel stems, participants are more likely to use the frequent suffix to convey a novel meaning.

```{r, echo = F, message = F}
#| label: tbl-prodresultsstem
#| tbl-cap: 'Results of the statistical models for the production task.'

prod_both_meanings_stems = bind_rows(fixefs_model_prod_both_stems, .id = "ID") %>%
  mutate(term = rep(c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Novel Stem', 'Type Frequency:Novel Meaning', 'Token Frequency:Novel Meaning', 'Type Frequency:Novel Stem', 'Token Frequency:Novel Stem', 'Novel Meaning:Novel Stem', 'Type Frequency:Novel Meaning:Novel Stem', 'Token Frequency:Novel Meaning:Novel Stem'), times = 1))


prod_both_meanings_stems = prod_both_meanings_stems %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(prod_both_meanings$meaning)

prod_both_meanings_stems %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 9, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '18em')
```

```{r, echo = F, out.width = '100%', fig.align = 'center', warning = F, message = F}
#| label: fig-prodresultsstem
#| fig-cap: "Plot of the statistical model estimates for the production task."


plot_prod_stem = plot_model(model_prod_both_stems, type = 'int',
                            axis.title = c('Condition', 'Frequency'),
                            axis.labels = "condition",
                            legend.title = 'Meaning',
                            title = "",
                            dot.size = 2,
                            line.size = 1,
                            colors = c('#298c8c', '#800074'))[[4]] 
plot_prod_stem[[11]]$linetype = 'Meaning'
plot_prod_stem[[11]]$shape = 'Meaning'

plot_prod_stem[[1]]$group_col = factor(plot_prod_stem[[1]]$group_col, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$group = factor(plot_prod_stem[[1]]$group, levels = c('novel', 'original'), labels = c('Novel Meaning', 'Original Meaning'))

plot_prod_stem[[1]]$facet = factor(plot_prod_stem[[1]]$facet, levels = c('novel', 'familiar'), labels = c('Novel Stem', 'Familiar Stem'))

#plot_prod_stem[[1]]$x

plot_prod_stem +
  scale_x_continuous(breaks = c(1, 2, 3), 
                     labels = c('Type Frequency', 'Token Frequency', 'Type-Token Frequency')) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

## Form Choice Task

In order to examine the effects of form-choice, we similarly ran a model analogous to the one we ran for the production task (@eq-prodmodels). In the context of the form-choice task, the dependent variable reflects whether participants chose the option with the frequent suffix or the one with the infrequent suffix.

The results of the form-choice task suggest that when participants are presented with both possible options, for both token-frequency and type-token frequency, participants are equally likely to choose the frequent suffix as the infrequent suffix. Interestingly, for the type-frequency condition, participants were less likely to choose the novel meaning than the original meaning, though the effect size is quite small and still close to chance.[^1]

[^1]: We ran an additional model with stem condition as a fixed-effect (along with its interaction with condition and meaning) and found no effect of stem condition.

```{r, echo = F, message = F}
#| label: tbl-2afcresults
#| tbl-cap: 'Results of the statistical models for the 2afc task.'
#| 
tafc_both_meanings = bind_rows(fixefs_model_2afc_both, .id = "ID") %>%
  mutate(term = rep(c('Intercept', 'Type Frequency', 'Token Frequency', 'Novel Meaning', 'Type Frequency:Novel', 'Token Frequency:Novel'), times = 1))


tafc_both_meanings = tafc_both_meanings %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))

#groupings=rle(tafc_both_meanings$meaning)

tafc_both_meanings %>%
  select(term, everything(), -ID) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 9, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '14em')

```

## Comprehension Task

Our results for the comprehension task are included below in @tbl-comprehensionresults. For the comprehension task, the dependent variable for our statistical analysis was whether the meaning that participants chose was novel or original. A positive estimate suggests that participants were more likely to choose the novel meaning, a negative estimate suggests that participants were more likely to choose the original meaning. For comprehension, our independent variables were different from the other tasks. Specifically, to maximize our power, we divided the data up based on whether the token-to-type ratio was 12-12, 12-3, or 3-3. By comparing the 12-3 condition to the 12-12 and 3-3 conditions, we can see how an increase in type-frequency and a decrease in token-frequency affect participants' choice of the novel or original meaning.

We treatment coded condition such that the intercept was the 12-3 condition. The model equation is included below in @eq-comprehensionmodel.

$$
\text{meaning} \sim \text{condition\_numeric} + (1 | \text{participant})
$$ {#eq-comprehensionmodel}

We find a meaningful effect of condition such increasing type-frequency to be equal to token frequency results in semantic entrenchment in comprehension. We also find that reducing token-frequency to be equal to type-frequency also results in semantic entrenchment. This suggests that it is the relationship between type-frequency and token-frequency (as opposed to simply the raw number of tokens or the raw number of types) that is relevant for semantic entrenchment in comprehension.[^2]

[^2]: Similar to form choice, we also ran a model with stem condition as a fixed-effect and found no effect.

```{r, echo = F, message = F}
#| label: tbl-comprehensionresults
#| tbl-cap: 'Results of the statistical models for the comprehension task.'
#| 

fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(term = rep(c('Intercept', '12-12', '3-3'), times = 1))


fixefs_model_comprehension_numeric = fixefs_model_comprehension_numeric %>%
  mutate(across(c(Estimate, Est.Error, Q2.5, Q97.5, `% Samples > 0`), as.numeric))


fixefs_model_comprehension_numeric %>%
  select(term, everything()) %>%
  rename(` ` = term) %>%
  kable(digits = 2, row.names = FALSE, booktabs = TRUE, format = "latex", escape = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"), font_size = 9, full_width = FALSE) %>%
  #group_rows(index = setNames(groupings$lengths, groupings$values), indent = TRUE) %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(1, bold = T, width = '20em')


```

<!--# there's probably a note about storage here -->

\clearpage
